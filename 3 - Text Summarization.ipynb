{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8b0767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (23.1.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (15.0.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (1.24.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (23.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (0.29.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (1.51.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (5.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in c:\\users\\deepblue\\appdata\\roaming\\python\\python310\\site-packages (0.6.1)\n",
      "Requirement already satisfied: keras-core in c:\\users\\deepblue\\appdata\\roaming\\python\\python310\\site-packages (from keras-nlp) (0.1.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-nlp) (1.24.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-nlp) (23.0)\n",
      "Requirement already satisfied: regex in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-nlp) (2022.10.31)\n",
      "Requirement already satisfied: rich in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-nlp) (13.4.2)\n",
      "Requirement already satisfied: tensorflow-text in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-nlp) (2.10.0)\n",
      "Requirement already satisfied: namex in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-core->keras-nlp) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-core->keras-nlp) (3.7.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-core->keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras-nlp) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\deepblue\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras-nlp) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-text->keras-nlp) (0.13.0)\n",
      "Requirement already satisfied: tensorflow<2.11,>=2.10.0 in c:\\users\\deepblue\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-text->keras-nlp) (2.10.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (23.1.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (15.0.6.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.29.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.51.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (5.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clean-text) (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\deepblue\\appdata\\roaming\\python\\python310\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\deepblue\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepblue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-gpu\n",
    "!pip3 install keras-nlp\n",
    "!pip3 install pandas\n",
    "!pip3 install clean-text\n",
    "!pip3 install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636a3235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "C:\\Users\\Deepblue\\AppData\\Local\\Temp\\ipykernel_28648\\3769467074.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d83017",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "TEXT_VOCAB_SIZE = 7700\n",
    "SUMM_VOCAB_SIZE = 3800\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec80a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "      <td>Olivia: Who are you voting for in this electio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "      <td>Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel...</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>Sam is confused, because he overheard Rick com...</td>\n",
       "      <td>Sam: hey  overheard rick say something\\r\\nSam:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                            summary  \\\n",
       "0  13818513  Amanda baked cookies and will bring Jerry some...   \n",
       "1  13728867  Olivia and Olivier are voting for liberals in ...   \n",
       "2  13681000  Kim may try the pomodoro technique recommended...   \n",
       "3  13730747  Edward thinks he is in love with Bella. Rachel...   \n",
       "4  13728094  Sam is confused, because he overheard Rick com...   \n",
       "\n",
       "                                                text  \n",
       "0  Amanda: I baked  cookies. Do you want some?\\r\\...  \n",
       "1  Olivia: Who are you voting for in this electio...  \n",
       "2  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...  \n",
       "3  Edward: Rachel, I think I'm in ove with Bella....  \n",
       "4  Sam: hey  overheard rick say something\\r\\nSam:...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_json(\"dataset/summarization/train.json\")\n",
    "train_df.columns = [\"id\", \"summary\", \"text\"]\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584e6ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      " Amanda: I baked  cookies. Do you want some?\r\n",
      "Jerry: Sure!\r\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "-----------------------------\n",
      "Summary:  Amanda baked cookies and will bring Jerry some tomorrow.\n"
     ]
    }
   ],
   "source": [
    "print(\"Text:\\n\",train_df[\"text\"][0])\n",
    "print(\"-----------------------------\")\n",
    "print(\"Summary: \", train_df[\"summary\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c95e9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_samples(text):\n",
    "    return clean(text=text, normalize_whitespace=True, no_urls=True, no_emoji=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d048011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "for i,j in zip(train_df.text, train_df.summary):\n",
    "    if len(i.split(\" \")) < MAX_SEQUENCE_LENGTH:\n",
    "        text_pairs.append((clean_samples(i),clean_samples(j)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b99de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    word_piece_ds.batch(1000).prefetch(2),\n",
    "    vocabulary_size=vocab_size,\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fbf6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "text_samples = [text_pair[0] for text_pair in text_pairs]\n",
    "text_vocab = train_word_piece(text_samples, TEXT_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "summ_samples = [text_pair[1] for text_pair in text_pairs]\n",
    "summ_vocab = train_word_piece(text_samples, SUMM_VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8452244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Tokens:  ['don', 'with', 'this', '##s', 'was', 'there', 'like', 'your', 'about', 'he']\n",
      "Summarization Tokens:  ['just', 'don', 'with', 'this', 'was', 'there', 'like', 'your', 'about', 'he']\n"
     ]
    }
   ],
   "source": [
    "print(\"Text Tokens: \", text_vocab[100:110])\n",
    "print(\"Summarization Tokens: \", summ_vocab[100:110])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae1ea19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6453, 3612)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_vocab), len(summ_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3d4e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "vocabulary=text_vocab, lowercase=False\n",
    ")\n",
    "summ_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "vocabulary=summ_vocab, lowercase=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c34b2cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sentence:  amanda: i baked cookies. do you want some?\n",
      "jerry: sure!\n",
      "amanda: i'll bring you tomorrow :-)\n",
      "Tokens:  tf.Tensor(\n",
      "[ 410   29   49 3453 1746   17   90   70  144  123   34  572   29  115\n",
      "    4  410   29   49   10   95  303   70  172   29   16   12], shape=(26,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b\"amanda : i baked cookies . do you want some ? jerry : sure ! amanda : i ' ll bring you tomorrow : - )\", shape=(), dtype=string)\n",
      "\n",
      "Summarization sentence:  amanda baked cookies and will bring jerry some tomorrow.\n",
      "Tokens:  tf.Tensor([ 421 2724  164 1898   74   99  316  593  123  177   17], shape=(11,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b'amanda baked cookies and will bring jerry some tomorrow .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "text_input_ex = text_pairs[0][0]\n",
    "text_tokens_ex = text_tokenizer.tokenize(text_input_ex)\n",
    "print(\"Text sentence: \",text_input_ex)\n",
    "print(\"Tokens: \", text_tokens_ex)\n",
    "print(\n",
    "\"Recovered text after detokenizing: \",\n",
    "text_tokenizer.detokenize(text_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "summ_input_ex = text_pairs[0][1]\n",
    "summ_tokens_ex = summ_tokenizer.tokenize(summ_input_ex)\n",
    "print(\"Summarization sentence: \", summ_input_ex)\n",
    "print(\"Tokens: \", summ_tokens_ex)\n",
    "print(\n",
    "\"Recovered text after detokenizing: \",\n",
    "summ_tokenizer.detokenize(summ_tokens_ex),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f26af1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(text, summ):\n",
    "    batch_size = tf.shape(summ)[0]\n",
    "    \n",
    "    text = text_tokenizer(text)\n",
    "    summ = summ_tokenizer(summ)\n",
    "    \n",
    "    # Pad `text` to `MAX_SEQUENCE_LENGTH`.\n",
    "    text_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    pad_value=text_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    text = text_start_end_packer(text)\n",
    "    \n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `summ` and pad it as well.\n",
    "    summ_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "    start_value=summ_tokenizer.token_to_id(\"[START]\"),\n",
    "    end_value=summ_tokenizer.token_to_id(\"[END]\"),\n",
    "    pad_value=summ_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    summ = summ_start_end_packer(summ)\n",
    "    \n",
    "    return (\n",
    "    {\n",
    "        \"encoder_inputs\": text,\n",
    "        \"decoder_inputs\": summ[:, :-1],\n",
    "    },\n",
    "        summ[:, 1:],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dfd30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(pairs):\n",
    "    source_texts, target_texts = zip(*pairs)\n",
    "    source_texts = list(source_texts)\n",
    "    target_texts = list(target_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((source_texts, target_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "train_ds = make_dataset(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cb41f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 150)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 150)\n",
      "targets.shape: (64, 150)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56c52aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "vocabulary_size=TEXT_VOCAB_SIZE,\n",
    "sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "embedding_dim=EMBED_DIM,\n",
    "mask_zero=True)(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS)(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b8ed38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "vocabulary_size=SUMM_VOCAB_SIZE,\n",
    "sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "embedding_dim=EMBED_DIM,\n",
    "mask_zero=True,)(decoder_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8c004df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "vocabulary_size=SUMM_VOCAB_SIZE,\n",
    "sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "embedding_dim=EMBED_DIM,\n",
    "mask_zero=True,)(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(SUMM_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "[\n",
    "    decoder_inputs,\n",
    "    encoded_seq_inputs,\n",
    "],\n",
    "    decoder_outputs,\n",
    ")\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "[encoder_inputs, decoder_inputs],\n",
    "decoder_outputs,\n",
    "name=\"transformer\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c930c4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, None, 256)   2009600     ['encoder_inputs[0][0]']         \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   1315072     ['token_and_position_embedding[0]\n",
      " erEncoder)                                                      [0]']                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 3800)   3566552     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,891,224\n",
      "Trainable params: 6,891,224\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc3a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "197/197 [==============================] - 20s 80ms/step - loss: 1.0061 - accuracy: 0.1914\n",
      "Epoch 2/40\n",
      "197/197 [==============================] - 16s 80ms/step - loss: 0.8428 - accuracy: 0.2462\n",
      "Epoch 3/40\n",
      "197/197 [==============================] - 16s 80ms/step - loss: 0.7894 - accuracy: 0.2674\n",
      "Epoch 4/40\n",
      "197/197 [==============================] - 16s 81ms/step - loss: 0.7464 - accuracy: 0.2885\n",
      "Epoch 5/40\n",
      "197/197 [==============================] - 15s 79ms/step - loss: 0.7019 - accuracy: 0.3143\n",
      "Epoch 6/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.6547 - accuracy: 0.3452\n",
      "Epoch 7/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.6072 - accuracy: 0.3760\n",
      "Epoch 8/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.5626 - accuracy: 0.4065\n",
      "Epoch 9/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.5205 - accuracy: 0.4363\n",
      "Epoch 10/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.4820 - accuracy: 0.4654\n",
      "Epoch 11/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.4454 - accuracy: 0.4952\n",
      "Epoch 12/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.4108 - accuracy: 0.5226\n",
      "Epoch 13/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.3785 - accuracy: 0.5507\n",
      "Epoch 14/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.3478 - accuracy: 0.5783\n",
      "Epoch 15/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.3218 - accuracy: 0.6016\n",
      "Epoch 16/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.2963 - accuracy: 0.6255\n",
      "Epoch 17/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.2728 - accuracy: 0.6480\n",
      "Epoch 18/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.2515 - accuracy: 0.6704\n",
      "Epoch 19/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.2320 - accuracy: 0.6892\n",
      "Epoch 20/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.2120 - accuracy: 0.7109\n",
      "Epoch 21/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.1965 - accuracy: 0.7275\n",
      "Epoch 22/40\n",
      "197/197 [==============================] - 15s 78ms/step - loss: 0.1829 - accuracy: 0.7430\n",
      "Epoch 23/40\n",
      "197/197 [==============================] - 15s 78ms/step - loss: 0.1710 - accuracy: 0.7553\n",
      "Epoch 24/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.1595 - accuracy: 0.7697\n",
      "Epoch 25/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.1456 - accuracy: 0.7873\n",
      "Epoch 26/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.1338 - accuracy: 0.8024\n",
      "Epoch 27/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.1230 - accuracy: 0.8166\n",
      "Epoch 28/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.1151 - accuracy: 0.8267\n",
      "Epoch 29/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.1072 - accuracy: 0.8364\n",
      "Epoch 30/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.1012 - accuracy: 0.8451\n",
      "Epoch 31/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0952 - accuracy: 0.8535\n",
      "Epoch 32/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0915 - accuracy: 0.8585\n",
      "Epoch 33/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0871 - accuracy: 0.8647\n",
      "Epoch 34/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0833 - accuracy: 0.8700\n",
      "Epoch 35/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0790 - accuracy: 0.8767\n",
      "Epoch 36/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0752 - accuracy: 0.8823\n",
      "Epoch 37/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0724 - accuracy: 0.8868\n",
      "Epoch 38/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0688 - accuracy: 0.8915\n",
      "Epoch 39/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0665 - accuracy: 0.8955\n",
      "Epoch 40/40\n",
      "197/197 [==============================] - 15s 77ms/step - loss: 0.0640 - accuracy: 0.8990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14526c25e70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4df8083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "    \n",
    "    # Tokenize the encoder input.\n",
    "    encoder_input_tokens = text_tokenizer(input_sentences).to_tensor(\n",
    "    shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "    \n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def next(prompt, cache, index):\n",
    "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        # Ignore hidden states for now; only needed for contrastive search.\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "    \n",
    "    # Build a prompt of length MAX_SEQUENCE_LENGTH with a start token and padding tokens.\n",
    "    length = MAX_SEQUENCE_LENGTH\n",
    "    start = tf.fill((batch_size, 1), summ_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill((batch_size, length - 1), summ_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat((start, pad), axis=-1)\n",
    "    \n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "    next,\n",
    "    prompt,\n",
    "    end_token_id=summ_tokenizer.token_to_id(\"[END]\"),\n",
    "    index=1,  # Start sampling after start token.\n",
    "    )\n",
    "    generated_sentences = summ_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0ae268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "Text: susan: are you as bored as i am?\n",
      "tom: i'm sleeping with eyes open so the prof won't realise\n",
      "linda: <photo_file>\n",
      "tom: nice drawings\n",
      "Prediction: susan , tom and linda are bored at the university lecture .\n",
      "Ground Truth: susan, tom and linda are bored at the university lecture. linda sends photo of her drawings.\n",
      "\n",
      "** Example 1 **\n",
      "Text: jeff: bro its been a while!\n",
      "jeff: whats up\n",
      "steve: hey what's up\n",
      "steve: ya idk what happened!\n",
      "steve: you haven't been around!\n",
      "jeff: ya well i was with my gf\n",
      "jeff: but we broke up\n",
      "steve: ah that's why bro\n",
      "steve: sad to hear that you broke up with your gf tho\n",
      "jeff: it was rough but it was the past\n",
      "jeff: wanna meet up after work?\n",
      "steve: no problem\n",
      "steve: at o'sheas?\n",
      "jeff: np!\n",
      "Prediction: jeff wasn ' t available much as he had a girfriend , but jeff decides to meet there after work .\n",
      "Ground Truth: jeff wasn't available much as he had a girfriend, but they split up. jeff and gary will meet at o'sheas after work.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts = [pair[0] for pair in text_pairs]\n",
    "original = [pair[1] for pair in text_pairs]\n",
    "for i in range(2):\n",
    "    choice = random.randint(0, len(test_texts))\n",
    "    input_sentence = test_texts[choice]\n",
    "    summarized = decode_sequences(tf.constant([input_sentence]))\n",
    "    summarized = summarized.numpy()[0].decode(\"utf-8\")\n",
    "    summarized = (\n",
    "    summarized.replace(\"[PAD]\", \"\")\n",
    "    .replace(\"[START]\", \"\")\n",
    "    .replace(\"[END]\", \"\")\n",
    "    .strip()\n",
    "    )\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(\"Text:\", input_sentence)\n",
    "    print(\"Prediction:\", summarized)\n",
    "    print(\"Ground Truth:\", original[choice])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae94221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
